{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EEG_based_Emotion_Recognition_Using_CNN_model(with_32_channels).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN7MrOOLX+35Ad/auP/2gnr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbo8RfXxXheR","executionInfo":{"status":"ok","timestamp":1661538427542,"user_tz":-300,"elapsed":5844,"user":{"displayName":"Zobia Ali","userId":"02678059922917028881"}},"outputId":"d580c75d-3688-4fec-bd45-b261f018c9f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/forrestbao/pyeeg.git\n","  Cloning https://github.com/forrestbao/pyeeg.git to /tmp/pip-req-build-yy7t6cc3\n","  Running command git clone -q https://github.com/forrestbao/pyeeg.git /tmp/pip-req-build-yy7t6cc3\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyeeg==0.4.4) (1.21.6)\n"]}],"source":["!pip install git+https://github.com/forrestbao/pyeeg.git\n","import numpy as np\n","import pyeeg as pe\n","import pickle as pickle\n","import pandas as pd\n","import math\n","\n","from sklearn import svm\n","from sklearn.preprocessing import normalize\n","\n","import os\n","import time"]},{"cell_type":"code","source":["import pandas as pd\n","import keras.backend as K\n","import numpy as np\n","import pandas as pd\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.models import Sequential\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from tensorflow.keras.utils import to_categorical \n","from keras.layers import Flatten\n","from keras.layers import Dense\n","import numpy as np\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n","from keras import backend as K\n","from keras.models import Model\n","import timeit\n","from keras.models import Sequential\n","from keras.layers.core import Flatten, Dense, Dropout\n","from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D\n","from tensorflow.keras.optimizers import SGD\n","#import cv2, numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"374VIWhfXmgc","executionInfo":{"status":"ok","timestamp":1661538427542,"user_tz":-300,"elapsed":5,"user":{"displayName":"Zobia Ali","userId":"02678059922917028881"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XtXSyYadXpkQ","executionInfo":{"status":"ok","timestamp":1661538449681,"user_tz":-300,"elapsed":22143,"user":{"displayName":"Zobia Ali","userId":"02678059922917028881"}},"outputId":"c168686c-a094-46be-8018-07b8bc286eb2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"id":"JOXuEM0JwJqI","executionInfo":{"status":"ok","timestamp":1661538449681,"user_tz":-300,"elapsed":5,"user":{"displayName":"Zobia Ali","userId":"02678059922917028881"}}},"source":["# data_preprocessed_python\n","os.getcwd()\n","os.chdir('/content/drive/My Drive')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"kCMT74t9wowG","executionInfo":{"status":"ok","timestamp":1661538449681,"user_tz":-300,"elapsed":4,"user":{"displayName":"Zobia Ali","userId":"02678059922917028881"}}},"source":["channel = [1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32] #32 Channels chosen to fit Emotiv Epoch+\n","band = [4,8,12,16,25,45] #5 bands THETA, ALPHA, ALPHA, BETA, BETA, BETA, GAMMA.\n","window_size = 256 #Averaging band power of 2 sec\n","step_size = 16 #Each 0.125 sec update once\n","sample_rate = 128 #Sampling rate of 128 Hz\n","subjectList = ['01','02','03']\n","#List of subjects"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"UD-WVHARwsXz","executionInfo":{"status":"ok","timestamp":1661538449682,"user_tz":-300,"elapsed":5,"user":{"displayName":"Zobia Ali","userId":"02678059922917028881"}}},"source":["def FFT_Processing (sub, channel, band, window_size, step_size, sample_rate):\n","    '''\n","    arguments:  string subject\n","                list channel indice\n","                list band\n","                int window size for FFT\n","                int step size for FFT\n","                int sample rate for FFT\n","    return:     void\n","    '''\n","    meta = []\n","    with open(\"/content/drive/My Drive/leading_ai/try/s\" + sub + '.dat', 'rb') as file:\n","\n","        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n","\n","        for i in range (0,40):\n","            # loop over 0-39 trails\n","            data = subject[\"data\"][i]\n","            labels = subject[\"labels\"][i]\n","            start = 0;\n","\n","            while start + window_size < data.shape[1]:\n","                meta_array = []\n","                meta_data = [] #meta vector for analysis\n","                for j in channel:\n","                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n","                    Y = pe.bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n","                    meta_data = meta_data + list(Y[0])\n","\n","                meta_array.append(np.array(meta_data))\n","                meta_array.append(labels)\n","\n","                meta.append(np.array(meta_array))    \n","                start = start + step_size\n","                \n","        meta = np.array(meta)\n","        np.save('/content/drive/My Drive/leading_ai/try/s' + sub, meta, allow_pickle=True, fix_imports=True)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWcHt5X2xDfw"},"source":["for subjects in subjectList:\n","    FFT_Processing (subjects, channel, band, window_size, step_size, sample_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dM2Bdu7oxIbE"},"source":["\n","data_training = []\n","label_training = []\n","data_testing = []\n","label_testing = []\n","\n","for subjects in subjectList:\n","\n","    with open('/content/drive/My Drive/leading_ai/try/s' + subjects + '.npy', 'rb') as file:\n","      sub = np.load(file,allow_pickle=True)\n","      for i in range (0,sub.shape[0]):\n","        if i % 5 == 0:\n","          data_testing.append(sub[i][0])\n","          label_testing.append(sub[i][1])\n","        else:\n","          data_training.append(sub[i][0])\n","          label_training.append(sub[i][1])\n","\n","np.save('/content/drive/My Drive/leading_ai/data_training', np.array(data_training), allow_pickle=True, fix_imports=True)\n","np.save('/content/drive/My Drive/leading_ai/label_training', np.array(label_training), allow_pickle=True, fix_imports=True)\n","print(\"training dataset:\", np.array(data_training).shape, np.array(label_training).shape)\n","\n","np.save('/content/drive/My Drive/leading_ai/data_testing', np.array(data_testing), allow_pickle=True, fix_imports=True)\n","np.save('/content/drive/My Drive/leading_ai/label_testing', np.array(label_testing), allow_pickle=True, fix_imports=True)\n","print(\"testing dataset:\", np.array(data_testing).shape, np.array(label_testing).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pnNNgiBN_HcS"},"source":["\n","with open('/content/drive/My Drive/leading_ai/data_training.npy', 'rb') as fileTrain:\n","    X  = np.load(fileTrain)\n","    \n","with open('/content/drive/My Drive/leading_ai/label_training.npy', 'rb') as fileTrainL:\n","    Y  = np.load(fileTrainL)\n","    \n","X = normalize(X)\n","Z = np.ravel(Y[:, [3]])\n","\n","Arousal_Train = np.ravel(Y[:, [0]])\n","Valence_Train = np.ravel(Y[:, [1]])\n","Domain_Train = np.ravel(Y[:, [2]])\n","Like_Train = np.ravel(Y[:, [3]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O-kXkIKo_3eY"},"source":["X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u7QbIvJF_7YH"},"source":["from keras.utils import to_categorical\n","y_train = to_categorical(Z)\n","y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vz6DyN6i4Ten"},"source":["y_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2nbzExFABT9"},"source":["x_train = np.array(X[:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ooCmmlGRAFkv"},"source":["\n","with open('/content/drive/My Drive/leading_ai/data_testing.npy', 'rb') as fileTrain:\n","    M  = np.load(fileTrain)\n","    \n","with open('/content/drive/My Drive/leading_ai/label_testing.npy', 'rb') as fileTrainL:\n","    N  = np.load(fileTrainL)\n","\n","M = normalize(M)\n","L = np.ravel(N[:, [3]])\n","\n","Arousal_Test = np.ravel(N[:, [0]])\n","Valence_Test = np.ravel(N[:, [1]])\n","Domain_Test = np.ravel(N[:, [2]])\n","Like_Test = np.ravel(N[:, [3]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4sxtvpQAOhK"},"source":["x_test = np.array(M[:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2RKm9KgASb4"},"source":["from keras.utils import to_categorical\n","y_test = to_categorical(L)\n","y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmwAfzADsRe1"},"source":["y_test[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKnp-c5EAVGJ"},"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","x_train = scaler.fit_transform(x_train)\n","x_test = scaler.fit_transform(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WC6VSZLUBCiS"},"source":["x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n","x_test = x_test.reshape(x_test.shape[0],x_test.shape[1], 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3CYMY96BE0i"},"source":["x_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V8NOrABgBWzf"},"source":["batch_size = 256\n","num_classes = 10\n","epochs = 200\n","input_shape=(x_train.shape[1], 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrpmQ4QWBc4_"},"source":["from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n","from keras.regularizers import l2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2B1ezppYCOhJ"},"source":["model = Sequential()\n","intput_shape=(x_train.shape[1], 1)\n","model.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\n","model.add(BatchNormalization())\n","model.add(MaxPooling1D(pool_size=(2)))\n","model.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling1D(pool_size=(2)))\n","model.add(Conv1D(64,kernel_size=3,padding = 'same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=(2)))\n","model.add(Flatten())\n","model.add(Dense(64, activation='tanh'))\n","model.add(Dropout(0.2))\n","model.add(Dense(32, activation='tanh'))\n","model.add(Dropout(0.2))\n","model.add(Dense(16, activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(num_classes, activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7abBpBSCY-o"},"source":["model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer='adam',\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaZ4RPIqCfYF"},"source":["history=model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,  \n","          verbose=1,validation_data=(x_test,y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JY7d6VDZCjaO"},"source":["score = model.evaluate(x_test, y_test, verbose=1)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LV_xGFKhDt67"},"source":["# list all data in history\n","print(history.history.keys())\n","import matplotlib.pyplot as plt\n","import numpy\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDQGSy8xFoob"},"source":["# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fgg6gD0BFnZS"},"source":["# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZwaY3SeKnJb"},"source":["plt.plot(history.history['val_accuracy'])\n","plt.plot(history.history['val_loss'])\n","plt.title('test model')\n","plt.ylabel('test accuracy')\n","plt.xlabel('test loss')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGR4wrOdL5DF"},"source":["y_pred=model.predict(x_test)\n","from sklearn.metrics import confusion_matrix\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9y5wCoinVRhi"},"source":["y_test[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2q13VADTXlQ9"},"source":["y_pred[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilOzv9RdVOtg"},"source":["y_test1=np.argmax(y_test, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pvkh6dE6XzNa"},"source":["y_pred=np.argmax(y_pred,axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLOOyigNaJke"},"source":["y_test1[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0f968lExaSRS"},"source":["y_pred[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"45fHlde5OtWn"},"source":["cmatrix=confusion_matrix(y_test1, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7p70V5JCRyvZ"},"source":["import seaborn as sns\n","figure = plt.figure(figsize=(8, 8))\n","sns.heatmap(cmatrix, annot=True,cmap=plt.cm.Blues)\n","plt.tight_layout()\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.show()"],"execution_count":null,"outputs":[]}]}